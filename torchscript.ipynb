{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stable_diffusion_pytorch import model_loader, pipeline\n",
    "from stable_diffusion_pytorch.samplers.k_lms import KLMSSampler\n",
    "from stable_diffusion_pytorch.tokenizer import Tokenizer\n",
    "import stable_diffusion_pytorch.util as util\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "import torch, sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch:  2.0.0+cpu\n",
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Device: \", device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "prompt = \"1girl, purple hair, genshin, high quality, masterpiece, raiden shogun, japanese, kimono\"\n",
    "prompts = [prompt]\n",
    "\n",
    "uncond_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n",
    "uncond_prompts = [uncond_prompt] if uncond_prompt else None\n",
    "uncond_prompts = uncond_prompts or [\"\"] * len(prompts)\n",
    "input_images = [Image.open(\"C:\\\\Users\\\\frank\\\\Documents\\\\카카오톡 받은 파일\\\\KakaoTalk_20220604_234317644.png\")]\n",
    "strength = 0.8\n",
    "do_cfg = True\n",
    "cfg_scale = 7.5\n",
    "height = 512\n",
    "width = 512\n",
    "n_inference_steps = 50\n",
    "seed = 42\n",
    "use_jit = True\n",
    "export = False and not use_jit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if use_jit:\n",
    "    models = dict()\n",
    "    for name in ['clip', 'decoder', 'diffusion', 'encoder']:\n",
    "        models[name] = torch.jit.load(f\"nvai-jit/{name}.pt\")\n",
    "        models[name].eval()\n",
    "else:\n",
    "    torch.jit.enable_onednn_fusion(True)\n",
    "    models = model_loader.preload_models(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = Tokenizer()\n",
    "    generator = torch.Generator(device='cpu')\n",
    "    generator.manual_seed(seed)\n",
    "    tokens = tokenizer.encode_batch(prompts)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "    cond_context = models['clip'](tokens)\n",
    "    uncond_tokens = tokenizer.encode_batch(uncond_prompts or [\"\"] * len(prompts))\n",
    "    uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n",
    "    uncond_context = models['clip'](uncond_tokens)\n",
    "    context = torch.cat([cond_context, uncond_context])\n",
    "\n",
    "    if export:\n",
    "        with torch.jit.optimized_execution(True):\n",
    "            clip_traced = torch.jit.trace(models['clip'], tokens)\n",
    "            clip_traced.eval()\n",
    "            clip_traced = torch.jit.freeze(clip_traced)\n",
    "        clip_traced.save(\"nvai-jit/clip.pt\")\n",
    "        del clip_traced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "del models['clip']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 77, 768])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512]) torch.Size([1, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sampler = KLMSSampler(n_inference_steps=n_inference_steps)\n",
    "    noise_shape = (len(prompts), 4, height // 8, width // 8)\n",
    "    processed_input_images = []\n",
    "    for input_image in input_images:\n",
    "        input_image = input_image.resize((width, height))\n",
    "        input_image = np.array(input_image)[:, :, :3]\n",
    "        input_image = torch.tensor(input_image, dtype=torch.float32)\n",
    "        input_image = util.rescale(input_image, (0, 255), (-1, 1))\n",
    "        processed_input_images.append(input_image)\n",
    "    input_images_tensor = torch.stack(processed_input_images).to(device)\n",
    "    input_images_tensor = util.move_channel(input_images_tensor, to=\"first\")\n",
    "\n",
    "    _, _, height, width = input_images_tensor.shape\n",
    "\n",
    "    encoder_noise = torch.randn(noise_shape, generator=generator, device=device)\n",
    "    print(input_images_tensor.shape, encoder_noise.shape)\n",
    "    latents = models['encoder'](input_images_tensor, encoder_noise)\n",
    "\n",
    "    latents_noise = torch.randn(noise_shape, generator=generator, device=device)\n",
    "    sampler.set_strength(strength=strength)\n",
    "    latents += latents_noise * sampler.initial_scale\n",
    "\n",
    "    if export:\n",
    "        with torch.jit.optimized_execution(True):\n",
    "            encoder_traced = torch.jit.trace(models['encoder'], (input_images_tensor, encoder_noise))\n",
    "            encoder_traced.eval()\n",
    "            encoder_traced = torch.jit.freeze(encoder_traced)\n",
    "        encoder_traced.save(\"nvai-jit/encoder.pt\")\n",
    "        del encoder_traced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del models['encoder']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latents.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampler.timesteps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampler.sigmas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    timesteps = tqdm(sampler.timesteps)\n",
    "    for i, timestep in enumerate(timesteps):\n",
    "        time_embedding = util.get_time_embedding(timestep).to(device)\n",
    "        input_latents = latents * sampler.get_input_scale()\n",
    "        if do_cfg:\n",
    "            input_latents = input_latents.repeat(2, 1, 1, 1)\n",
    "        output = models['diffusion'](input_latents, context, time_embedding)\n",
    "        if export and i == 0:\n",
    "            with torch.jit.optimized_execution(True):\n",
    "                diffusion_traced = torch.jit.trace(models['diffusion'], (input_latents, context, time_embedding))\n",
    "                diffusion_traced.eval()\n",
    "                diffusion_traced = torch.jit.freeze(diffusion_traced)\n",
    "            diffusion_traced.save(\"nvai-jit/diffusion.pt\")\n",
    "            del diffusion_traced\n",
    "            print(\"Exported diffusion model.\")\n",
    "        if do_cfg:\n",
    "            output_cond, output_uncond = output.chunk(2)\n",
    "            output = cfg_scale * (output_cond - output_uncond) + output_uncond\n",
    "        latents = sampler.step(latents, output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del models['diffusion']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latents.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = models['decoder'](latents)\n",
    "    print(res.shape)\n",
    "    images = util.rescale(res, (-1, 1), (0, 255), clamp=True)\n",
    "    images = util.move_channel(images, to=\"last\")\n",
    "    images = images.to('cpu', torch.uint8).numpy()\n",
    "    results = [Image.fromarray(image) for image in images]\n",
    "    if export:\n",
    "        with torch.jit.optimized_execution(True):\n",
    "            decoder_traced = torch.jit.trace(models['decoder'], latents)\n",
    "            decoder_traced.eval()\n",
    "            decoder_traced = torch.jit.freeze(decoder_traced)\n",
    "        decoder_traced.save(\"nvai-jit/decoder.pt\")\n",
    "        del decoder_traced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = np.floor((res.numpy()+1)*127.5)\n",
    "t = t.astype(np.uint8)\n",
    "t = t[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = t.transpose(1,2,0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image.fromarray(t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del models['decoder']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for image in results:\n",
    "    display(image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
